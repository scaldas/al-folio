@inproceedings{caldas2021understanding,
  title={Understanding Clinical Collaborations Through Federated Classifier Selection},
  author={Caldas, Sebastian and Yoon, Joo Heung and Pinsky, Michael R and Clermont, Gilles and Dubrawski, Artur},
  booktitle={Machine Learning for Healthcare Conference},
  pages={126--145},
  year={2021},
  organization={PMLR},
  abbr={MLHC},
  category={conference},
  selected={true},
  html={https://proceedings.mlr.press/v149/caldas21a},
  pdf={https://proceedings.mlr.press/v149/caldas21a/caldas21a.pdf},
  poster={frcls_poster.pdf},
  slides={frcls_slides.pdf},
  talk={https://www.youtube.com/watch?v=iqxFnpVACkc},
  abstract={Deriving true clinical utility from models trained on multiple hospitals’ data is a key challenge in the adoption of Federated Learning (FL) systems in support of clinical collaborations. When utility is equated to predictive power, population heterogeneity between centers becomes a key bottleneck in training performant models. Nevertheless, there are other aspects to clinical utility that have frequently been overlooked in this context. Among them, we argue for the importance of understanding how a collaboration may be affecting the quality of a center’s predictions. Insights into how and when external knowledge is being useful can lead to strategic decisions by stakeholders, such as better allocation of local resources or even identifying best practices outside of the current organization. We take a step towards deriving such utility through FedeRated CLassifier Selection (FRCLS, pronounced “freckles”): an algorithm that reuses classifiers trained in outside institutions. It identifies regions of the feature space where the collaborators’ models will outperform the local center’s classifier, and can provide interpretable rules to describe these regions of beneficial expertise. We apply FRCLS to a sepsis prediction task in two different hospital systems, demonstrating its benefits in terms of understanding the types of patients for which the collaboration is useful and reasoning about the strategic decisions that may stem out of these analyses.}
}

@inproceedings{caldas2021using,
  title={Using Machine Learning to Support Transfer of Best Practices in Healthcare},
  author={Caldas, Sebastian and Chen, Jieshi and Dubrawski, Artur},
  booktitle={AMIA Annual Symposium Proceedings},
  year={2021},
  organization={American Medical Informatics Association},
  abbr={AMIA},
  category={conference},
  selected={false},
  abstract={The adoption of best practices has been shown to increase performance in healthcare institutions and is consistently demanded by both patients, payers, and external overseers. Nevertheless, transferring practices between healthcare organizations is a challenging and underexplored task. In this paper, we take a step towards enabling the transfer of best practices by identifying the likely beneficial opportunities for such transfer.  Specifically, we analyze the output of machine learning models trained at different organizations with the aims of (i) detecting the opportunity for the transfer of best practices, and (ii) providing a stop-gap solution while the actual transfer process takes place. We show the benefits of this methodology on a dataset of medical inpatient claims, demonstrating our ability to identify practice gaps and to support the transfer processes that address these gaps.}
}

@inproceedings{li2019differentially,
  title={Differentially Private Meta-Learning},
  author={Li, Jeffrey and Khodak, Mikhail and Caldas, Sebastian and Talwalkar, Ameet},
  booktitle={International Conference on Learning Representations},
  year={2019},
  abbr={ICLR},
  category ={conference},
  selected ={false},
  html={https://openreview.net/forum?id=rJgqMRVYvr},
  arxiv={1909.05830},
  pdf={https://arxiv.org/pdf/1909.05830.pdf},
  abstract={Parameter-transfer is a well-known and versatile approach for meta-learning, with applications including few-shot learning, federated learning, and reinforcement learning. However, parameter-transfer algorithms often require sharing models that have been trained on the samples from specific tasks, thus leaving the task-owners susceptible to breaches of privacy. We conduct the first formal study of privacy in this setting and formalize the notion of task-global differential privacy as a practical relaxation of more commonly studied threat models. We then propose a new differentially private algorithm for gradient-based parameter transfer that not only satisfies this privacy requirement but also retains provable transfer learning guarantees in convex settings. Empirically, we apply our analysis to the problems of federated learning with personalization and few-shot classification, showing that allowing the relaxation to task-global privacy from the more commonly studied notion of local privacy leads to dramatically increased performance in recurrent neural language modeling and image classification.}
}

@article{caldas2018expanding,
  title={Expanding the reach of federated learning by reducing client resource requirements},
  author={Caldas, Sebastian and Kone{\v{c}}ny, Jakub and McMahan, H Brendan and Talwalkar, Ameet},
  journal={Workshop on Federated Learning for Data Privacy and Confidentiality, FL-NeurIPS},
  year={2019},
  abbr={FL-NeurIPS},
  category ={workshop},
  selected ={false},
  arxiv={1812.07210},
  pdf={https://arxiv.org/pdf/1812.07210.pdf},
  poster={friendlyfl_poster.pdf},
  slides={friendlyfl_slides.pdf},
  abstract={Communication on heterogeneous edge networks is a fundamental bottleneck in Federated Learning (FL), restricting both model capacity and user participation. To address this issue, we introduce two novel strategies to reduce communication costs: (1) the use of lossy compression on the global model sent server-to-client; and (2) Federated Dropout, which allows users to efficiently train locally on smaller subsets of the global model and also provides a reduction in both client-to-server communication and local computation. We empirically show that these strategies, combined with existing compression approaches for client-to-server communication, collectively provide up to a 14× reduction in server-to-client communication, a 1.7x reduction in local computation, and a 28× reduction in upload communication, all without degrading the quality of the final model. We thus comprehensively reduce FL's impact on client device resources, allowing higher capacity models to be trained, and a more diverse set of users to be reached.}
}

@article{caldas2018leaf,
  title={LEAF: A benchmark for federated settings},
  author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal={Workshop on Federated Learning for Data Privacy and Confidentiality, FL-NeurIPS},
  year={2019},
  abbr={FL-NeurIPS},
  category ={workshop},
  selected ={true},
  website={https://leaf.cmu.edu},
  arxiv={1812.01097},
  pdf={https://arxiv.org/pdf/1812.01097.pdf},
  code={https://github.com/TalwalkarLab/leaf},
  poster={leaf_poster.pdf},
  abstract ={Modern federated networks, such as those comprised of wearable devices, mobile phones, or autonomous vehicles, generate massive amounts of data each day. This wealth of data can help to learn models that can improve the user experience on each device. However, the scale and heterogeneity of federated data presents new challenges in research areas such as federated learning, meta-learning, and multi-task learning. As the machine learning community begins to tackle these challenges, we are at a critical time to ensure that developments made in these areas are grounded with realistic benchmarks. To this end, we propose LEAF, a modular benchmarking framework for learning in federated settings. LEAF includes a suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.}
}

@article{caldas2018federated,
  title={Federated kernelized multi-task learning},
  author={Caldas, Sebastian and Smith, Virginia and Talwalkar, Ameet},
  journal={SysML Conf},
  year={2018},
  abbr={SysML},
  category={workshop},
  selected={false},
  pdf={https://mlsys.org/Conferences/2019/doc/2018/30.pdf},
  poster={fedkmtl_poster.pdf},
  abstract={Federated learning poses new statistical and systems challenges in the training of machine learning models over distributed networks of devices. In this ongoing work, we develop a state of the art MTL federated system that bypasses the modelling limitations of previous efforts through the inclusion of non-linear mappings in its formulation. We address the new issues that arise due to this inclusion and that are associated with the particulars of the federated scenario, such as communication and storage costs, introducing this way the first fully practical kernelized federated framework.}
}